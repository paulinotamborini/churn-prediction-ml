# ğŸ“ Estructura del Proyecto - Churn Prediction

## Estructura Recomendada

```
churn-prediction/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw/                          # Datos originales sin procesar
â”‚   â”‚   â””â”€â”€ WA_Fn-UseC_-Telco-Customer-Churn.csv
â”‚   â”‚
â”‚   â””â”€â”€ processed/                    # Datos procesados y listos para modelado
â”‚       â”œâ”€â”€ X_train.csv
â”‚       â”œâ”€â”€ X_test.csv
â”‚       â”œâ”€â”€ X_train_scaled.csv
â”‚       â”œâ”€â”€ X_test_scaled.csv
â”‚       â”œâ”€â”€ y_train.csv
â”‚       â””â”€â”€ y_test.csv
â”‚
â”œâ”€â”€ notebooks/                        # Jupyter notebooks para anÃ¡lisis
â”‚   â”œâ”€â”€ 01_eda.ipynb
â”‚   â”œâ”€â”€ 02_categorical_analysis.ipynb
â”‚   â”œâ”€â”€ 03_feature_engineering.ipynb
â”‚   â”œâ”€â”€ 04_modeling.ipynb
â”‚   â””â”€â”€ 05_optimization.ipynb (prÃ³ximo)
â”‚
â”œâ”€â”€ models/                           # Modelos entrenados
â”‚   â”œâ”€â”€ logistic_regression_model.pkl
â”‚   â”œâ”€â”€ random_forest_model.pkl
â”‚   â”œâ”€â”€ gradient_boosting_model.pkl
â”‚   â”œâ”€â”€ xgboost_model.pkl
â”‚   â”œâ”€â”€ scaler.pkl
â”‚   â””â”€â”€ model_comparison_results.csv
â”‚
â”œâ”€â”€ src/                              # CÃ³digo fuente (opcional para producciÃ³n)
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ preprocessing.py
â”‚   â”œâ”€â”€ features.py
â”‚   â”œâ”€â”€ model.py
â”‚   â””â”€â”€ predict.py
â”‚
â”œâ”€â”€ reports/                          # Reportes y visualizaciones
â”‚   â””â”€â”€ figures/
â”‚
â”œâ”€â”€ requirements.txt                  # Dependencias del proyecto
â”œâ”€â”€ README.md                         # DocumentaciÃ³n del proyecto
â””â”€â”€ .gitignore                        # Archivos a ignorar en git

```

## ğŸ“‹ DescripciÃ³n de Carpetas

### ğŸ“‚ `data/`
- **`raw/`**: Datos originales sin modificar. Nunca edites estos archivos.
- **`processed/`**: Datos limpios, transformados y listos para machine learning.

### ğŸ““ `notebooks/`
- Notebooks de Jupyter para anÃ¡lisis exploratorio, experimentaciÃ³n y visualizaciÃ³n.
- Numerados para seguir el flujo del proyecto.

### ğŸ¤– `models/`
- Modelos entrenados guardados como archivos `.pkl`
- Transformadores (scalers, encoders, etc.)
- Resultados de comparaciÃ³n de modelos

### ğŸ’» `src/`
- CÃ³digo Python reutilizable y modularizado
- Scripts para poner el modelo en producciÃ³n
- Ãštil cuando quieres desplegar el modelo como API o servicio

### ğŸ“Š `reports/`
- Reportes finales, presentaciones
- GrÃ¡ficos y visualizaciones para stakeholders

## ğŸ¯ Flujo de Trabajo

1. **EDA** â†’ `notebooks/01_eda.ipynb`
   - Carga datos desde `data/raw/`
   - AnÃ¡lisis exploratorio inicial

2. **Feature Engineering** â†’ `notebooks/03_feature_engineering.ipynb`
   - Crea nuevas variables
   - Guarda datasets procesados en `data/processed/`
   - Guarda scaler en `models/`

3. **Modelado** â†’ `notebooks/04_modeling.ipynb`
   - Carga datos desde `data/processed/`
   - Entrena mÃºltiples modelos
   - Guarda modelos en `models/`

4. **OptimizaciÃ³n** â†’ `notebooks/05_optimization.ipynb` (prÃ³ximo)
   - Hyperparameter tuning
   - Interpretabilidad

## ğŸš€ Ventajas de Esta Estructura

âœ… **OrganizaciÃ³n**: Cada tipo de archivo en su lugar
âœ… **Reproducibilidad**: FÃ¡cil seguir el flujo del proyecto
âœ… **ColaboraciÃ³n**: Otros pueden entender rÃ¡pidamente el proyecto
âœ… **Escalabilidad**: FÃ¡cil agregar nuevos componentes
âœ… **Git-friendly**: Puedes hacer `.gitignore` de `data/` y `models/` para no subir archivos pesados

## ğŸ“ Comandos Ãštiles

```bash
# Crear estructura de carpetas
mkdir -p data/raw data/processed models notebooks src reports/figures

# Mover datos crudos
mv WA_Fn-UseC_-Telco-Customer-Churn.csv data/raw/

# Git - ignorar archivos pesados
echo "data/" >> .gitignore
echo "models/*.pkl" >> .gitignore
echo ".ipynb_checkpoints/" >> .gitignore
```

## ğŸ”„ PrÃ³ximos Pasos

1. âœ… EDA bÃ¡sico
2. âœ… AnÃ¡lisis categÃ³rico
3. âœ… Feature Engineering
4. âœ… Modelado
5. â³ OptimizaciÃ³n de hiperparÃ¡metros
6. â³ Interpretabilidad (SHAP)
7. â³ Deployment (API con FastAPI/Flask)

---

**Nota**: Esta es la estructura estÃ¡ndar de la industria para proyectos de Data Science y Machine Learning.
